{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2d4e6bc",
   "metadata": {},
   "source": [
    "## 基于YOLO-V5的目标检测推理与Openvino优化\n",
    "\n",
    "该示例脚本演示如何基于YOLO-V5模型在自定义的数据集上训练目标检测模型，以及利用训练完后的目标检测模型进行前向推理的详细步骤。\n",
    "最后还演示利用Intel Openvino编译优化YOLO-V5模型的过程。\n",
    "\n",
    "Kernel选择：conda_pytorch_latest_p36 \n",
    "PyTorch Version: 1.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932d323b",
   "metadata": {},
   "source": [
    "#### 步骤一：基于官方yolov5在自定义的数据集上训练目标检测模型\n",
    "\n",
    "- 数据集路径：s3://neo-models-zoo/datasets/widerperson.zip\n",
    "- 数据集介绍：http://www.cbsr.ia.ac.cn/users/sfzhang/widerperson/\n",
    "- YOLO5 Repo: https://github.com/ultralytics/yolov5.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1da8d7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0+cu102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0fd9a916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n",
      "remote: Enumerating objects: 10036, done.\u001b[K\n",
      "remote: Total 10036 (delta 0), reused 0 (delta 0), pack-reused 10036\u001b[K\n",
      "Receiving objects: 100% (10036/10036), 10.38 MiB | 45.42 MiB/s, done.\n",
      "Resolving deltas: 100% (6943/6943), done.\n"
     ]
    }
   ],
   "source": [
    "! rm -rf yolov5\n",
    "! rm -rf datasets\n",
    "! git clone https://github.com/ultralytics/yolov5.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b9b8c6fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib>=3.2.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (3.3.4)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (1.19.5)\n",
      "Requirement already satisfied: opencv-python>=4.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (4.5.1.48)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (8.3.2)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (5.4.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r requirements.txt (line 9)) (2.26.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r requirements.txt (line 10)) (1.5.4)\n",
      "Requirement already satisfied: torch>=1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r requirements.txt (line 11)) (1.10.0)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r requirements.txt (line 12)) (0.11.1)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r requirements.txt (line 13)) (4.61.1)\n",
      "Requirement already satisfied: tensorboard>=2.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r requirements.txt (line 16)) (2.7.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r requirements.txt (line 20)) (1.1.5)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r requirements.txt (line 21)) (0.11.2)\n",
      "Requirement already satisfied: thop in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r requirements.txt (line 36)) (0.0.31.post2005241907)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (3.3)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch>=1.7.0->-r requirements.txt (line 11)) (3.10.0.2)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch>=1.7.0->-r requirements.txt (line 11)) (0.8)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.19.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.4.6)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.36.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.3.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.42.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (2.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.8.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.30.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (58.5.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas>=1.1.4->-r requirements.txt (line 20)) (2021.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from absl-py>=0.4->tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 16)) (4.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.1.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\r\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! cd yolov5 && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c5e1cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5m.pt, cfg=, data=factory.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=30, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/Gaowei-Xu/yolov5 ✅\n",
      "YOLOv5 🚀 v6.0-111-g040e0f9 torch 1.10.0+cu102 CUDA:0 (Tesla T4, 15110MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs (RECOMMENDED)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\n",
      "WARNING: Dataset not found, nonexistent paths: ['/home/ec2-user/SageMaker/datasets/factory/val.txt']\n",
      "Downloading https://neo-models-zoo.s3.amazonaws.com/datasets/factory.zip to factory.zip...\n",
      "100%|██████████████████████████████████████| 1.61G/1.61G [00:43<00:00, 39.8MB/s]\n",
      "Dataset autodownload success, saved to ../datasets\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5m.pt to yolov5m.pt...\n",
      "100%|███████████████████████████████████████| 40.7M/40.7M [00:00<00:00, 172MB/s]\n",
      "\n",
      "Overriding model.yaml nc=80 with nc=14\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n",
      "  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n",
      "  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n",
      "  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n",
      "  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n",
      "  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n",
      "  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n",
      "  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n",
      "  8                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n",
      "  9                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n",
      " 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n",
      " 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n",
      " 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n",
      " 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  2   4134912  models.common.C3                        [768, 768, 2, False]          \n",
      " 24      [17, 20, 23]  1     76779  models.yolo.Detect                      [14, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n",
      "Model Summary: 369 layers, 20923851 parameters, 20923851 gradients, 48.2 GFLOPs\n",
      "\n",
      "Transferred 475/481 items from yolov5m.pt\n",
      "Scaled weight_decay = 0.0005\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 79 weight, 82 weight (no decay), 82 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../datasets/factory/train' images and labels...440 found, 0 mis\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: ../datasets/factory/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/factory/val' images and labels...87 found, 0 missing,\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: ../datasets/factory/val.cache\n",
      "Plotting labels to runs/train/exp/labels.jpg... \n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.85 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      0/29     5.84G    0.1235   0.05952   0.07125        78       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634   0.000243     0.0128   0.000134   2.51e-05\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      1/29      7.3G    0.1106   0.07526   0.06877        94       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634     0.0025       0.11    0.00375   0.000538\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      2/29      7.3G   0.09726   0.07635   0.06628       106       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634     0.0406       0.17     0.0358    0.00764\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      3/29      7.3G   0.08047   0.06296   0.06321        95       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634      0.186      0.346      0.139     0.0374\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      4/29      7.3G    0.0677   0.05084   0.05778        67       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634      0.344      0.427      0.313      0.111\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      5/29      7.3G   0.06331   0.04848    0.0535        80       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634      0.462      0.398      0.317     0.0951\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      6/29      7.3G   0.07564   0.04364   0.05071        43       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634      0.386      0.356       0.26     0.0792\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      7/29      7.3G   0.07198   0.04291   0.04617        93       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634      0.343      0.525       0.37      0.117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      8/29      7.3G   0.06803   0.04191   0.04419        87       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634      0.611      0.399      0.296      0.111\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      9/29      7.3G   0.06712     0.041   0.04095        98       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634      0.349      0.673      0.543      0.247\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     13/29      7.3G   0.05639    0.0386   0.03009        68       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634      0.544      0.702      0.661      0.297\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     14/29      7.3G    0.0558   0.03856   0.02825        95       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634      0.645      0.723      0.738      0.352\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     15/29      7.3G   0.05253    0.0378   0.02466        78       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634      0.661      0.718      0.743      0.344\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     16/29      7.3G   0.05061   0.03509   0.02363        90       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634      0.639      0.746      0.724      0.346\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     17/29      7.3G   0.05046   0.03527   0.02185       112       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634      0.683      0.753      0.754      0.386\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     18/29      7.3G   0.05571   0.03752   0.02119       103       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634      0.862      0.584      0.752      0.377\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     19/29      7.3G    0.0529   0.03534   0.02014        97       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634      0.774      0.741      0.787      0.327\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     20/29      7.3G   0.05069   0.03496   0.01894        63       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634      0.689      0.733      0.733      0.314\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     21/29      7.3G   0.04861   0.03587   0.01828        88       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634      0.651       0.75      0.731      0.354\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     22/29      7.3G   0.04543   0.03468   0.01735        91       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634      0.816      0.804      0.836      0.485\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     23/29      7.3G   0.04396   0.03374   0.01626       132       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634      0.878      0.832      0.848      0.523\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     24/29      7.3G    0.0442   0.03396    0.0161        79       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634      0.831       0.84      0.836      0.497\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     25/29      7.3G   0.04076   0.03253    0.0153        98       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634       0.87       0.83      0.841      0.445\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     26/29      7.3G   0.03985   0.03234   0.01499       108       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634       0.95      0.838       0.85      0.565\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     27/29      7.3G   0.03758   0.03287   0.01436       102       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634      0.898      0.832      0.849      0.489\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     28/29      7.3G   0.03698   0.03323   0.01406        84       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634      0.948      0.836      0.853      0.522\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     29/29      7.3G     0.038   0.03288   0.01354        98       640: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634      0.949      0.843      0.852      0.483\n",
      "\n",
      "30 epochs completed in 1.046 hours.\n",
      "Optimizer stripped from runs/train/exp/weights/last.pt, 42.3MB\n",
      "Optimizer stripped from runs/train/exp/weights/best.pt, 42.3MB\n",
      "\n",
      "Validating runs/train/exp/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model Summary: 290 layers, 20905467 parameters, 0 gradients, 48.1 GFLOPs\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         87        634       0.95      0.838       0.85      0.565\n",
      "             2D_CODE         87         84      0.991          1      0.995      0.747\n",
      "             Caution         87          3          1          0    0.00494    0.00333\n",
      "                  3C         87         46      0.955      0.927      0.981      0.668\n",
      "                 EAC         87         65      0.982          1      0.995      0.636\n",
      "                 UL2         87         12       0.94          1      0.995      0.649\n",
      "                WEEE         87         58      0.988      0.983      0.985       0.63\n",
      "                  KC         87         60      0.971      0.983      0.974      0.581\n",
      "                ATEX         87         49          1      0.837      0.968      0.561\n",
      "                 FM2         87         10          1          0     0.0454     0.0339\n",
      "            Failsafe         87          4      0.871          1      0.995      0.746\n",
      "                 RCM         87         61       0.92          1      0.995      0.672\n",
      "                  FM         87         50      0.827          1      0.977      0.672\n",
      "                  CE         87         83      0.956          1      0.995      0.672\n",
      "                 UL1         87         49      0.897          1      0.995      0.645\n",
      "Results saved to \u001b[1mruns/train/exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 训练yolov5s检测模型\n",
    "! cd yolov5 && python3 train.py --data factory.yaml --weights yolov5m.pt --img 640 --epochs 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c5dd32",
   "metadata": {},
   "source": [
    "#### 步骤二：将yolov5模型转化为Openvino IR格式\n",
    "\n",
    "Reference：https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d920ccf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx==1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from onnx==1.10.0) (1.19.5)\n",
      "Requirement already satisfied: protobuf in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from onnx==1.10.0) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from onnx==1.10.0) (3.10.0.2)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from onnx==1.10.0) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: install in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.3.5)\n",
      "Requirement already satisfied: openvino-dev==2021.4.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2021.4.2)\n",
      "Requirement already satisfied: pillow>=8.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (8.3.2)\n",
      "Requirement already satisfied: progress>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (1.6)\n",
      "Requirement already satisfied: pandas~=1.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (1.1.5)\n",
      "Requirement already satisfied: tokenizers>=0.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (0.11.0)\n",
      "Requirement already satisfied: requests>=2.25.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (2.26.0)\n",
      "Requirement already satisfied: fast-ctc-decode>=0.2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (0.3.0)\n",
      "Requirement already satisfied: openvino==2021.4.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (2021.4.2)\n",
      "Requirement already satisfied: nltk>=3.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (3.6.2)\n",
      "Requirement already satisfied: rawpy>=0.16.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (0.16.0)\n",
      "Requirement already satisfied: texttable~=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (1.6.4)\n",
      "Requirement already satisfied: pydicom>=2.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.54.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (4.61.1)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (0.7.1)\n",
      "Requirement already satisfied: numpy<1.20,>=1.16.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (1.19.5)\n",
      "Requirement already satisfied: opencv-python==4.5.* in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (4.5.1.48)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (5.4.1)\n",
      "Requirement already satisfied: shapely>=1.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (1.8.0)\n",
      "Requirement already satisfied: hyperopt~=0.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (0.1.2)\n",
      "Requirement already satisfied: jstyleson~=0.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (0.0.2)\n",
      "Requirement already satisfied: networkx~=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (2.5)\n",
      "Requirement already satisfied: editdistance>=0.5.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (0.6.0)\n",
      "Requirement already satisfied: scikit-image>=0.17.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (0.17.2)\n",
      "Requirement already satisfied: addict>=2.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (2.4.0)\n",
      "Requirement already satisfied: py-cpuinfo>=7.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (8.0.0)\n",
      "Requirement already satisfied: nibabel>=3.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (3.2.1)\n",
      "Requirement already satisfied: parasail>=1.2.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (1.2.4)\n",
      "Requirement already satisfied: scipy~=1.5.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (1.5.4)\n",
      "Requirement already satisfied: sentencepiece>=0.1.95 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (0.1.96)\n",
      "Requirement already satisfied: scikit-learn>=0.24.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (0.24.2)\n",
      "Requirement already satisfied: yamlloader>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from openvino-dev==2021.4.2) (1.1.0)\n",
      "Requirement already satisfied: pymongo in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from hyperopt~=0.1.2->openvino-dev==2021.4.2) (4.0.1)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from hyperopt~=0.1.2->openvino-dev==2021.4.2) (0.18.2)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from hyperopt~=0.1.2->openvino-dev==2021.4.2) (1.16.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from networkx~=2.5->openvino-dev==2021.4.2) (5.0.9)\n",
      "Requirement already satisfied: packaging>=14.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from nibabel>=3.2.1->openvino-dev==2021.4.2) (21.2)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from nltk>=3.5->openvino-dev==2021.4.2) (8.0.1)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from nltk>=3.5->openvino-dev==2021.4.2) (1.0.1)\n",
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from nltk>=3.5->openvino-dev==2021.4.2) (2021.4.4)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=14.3->nibabel>=3.2.1->openvino-dev==2021.4.2) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas~=1.1.5->openvino-dev==2021.4.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas~=1.1.5->openvino-dev==2021.4.2) (2021.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.25.1->openvino-dev==2021.4.2) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.25.1->openvino-dev==2021.4.2) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.25.1->openvino-dev==2021.4.2) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.25.1->openvino-dev==2021.4.2) (2.0.7)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-image>=0.17.2->openvino-dev==2021.4.2) (3.3.4)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-image>=0.17.2->openvino-dev==2021.4.2) (2.9.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-image>=0.17.2->openvino-dev==2021.4.2) (2020.10.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-image>=0.17.2->openvino-dev==2021.4.2) (1.1.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.17.2->openvino-dev==2021.4.2) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.17.2->openvino-dev==2021.4.2) (0.10.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn>=0.24.1->openvino-dev==2021.4.2) (2.1.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from click->nltk>=3.5->openvino-dev==2021.4.2) (4.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->click->nltk>=3.5->openvino-dev==2021.4.2) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->click->nltk>=3.5->openvino-dev==2021.4.2) (3.10.0.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install onnx==1.10.0\n",
    "! pip install install openvino-dev==2021.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b0127a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['runs/train/exp/weights/best.pt'], imgsz=[640], batch_size=1, device=cpu, half=False, inplace=False, train=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['openvino']\n",
      "YOLOv5 🚀 v6.0-111-g040e0f9 torch 1.10.0+cu102 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 290 layers, 20905467 parameters, 0 gradients, 48.1 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from runs/train/exp/weights/best.pt (42.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.10.0...\n",
      "/home/ec2-user/SageMaker/yolov5/models/yolo.py:57: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.onnx_dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success, saved as runs/train/exp/weights/best.onnx (84.1 MB)\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m run --dynamic ONNX model inference with: 'python detect.py --weights runs/train/exp/weights/best.onnx'\n",
      "\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m starting export with openvino 2021.4.2-3976-0943ed67223-refs/pull/539/head...\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m export success, saved as runs/train/exp/weights/best_openvino_model/ (84.4 MB)\n",
      "\n",
      "Export complete (22.91s)\n",
      "Results saved to \u001b[1m/home/ec2-user/SageMaker/yolov5/runs/train/exp/weights\u001b[0m\n",
      "Visualize with https://netron.app\n"
     ]
    }
   ],
   "source": [
    "! cd yolov5 && python3 export.py --weights runs/train/exp/weights/best.pt --img 640 --batch 1 --include openvino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2c84b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best.bin  best.mapping\tbest.xml\n",
      "81M\t/home/ec2-user/SageMaker/yolov5/runs/train/exp/weights/best_openvino_model\n"
     ]
    }
   ],
   "source": [
    "! ls /home/ec2-user/SageMaker/yolov5/runs/train/exp/weights/best_openvino_model\n",
    "! du -h /home/ec2-user/SageMaker/yolov5/runs/train/exp/weights/best_openvino_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69132fa6",
   "metadata": {},
   "source": [
    "#### 步骤三：前向推理测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27e7c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return im, ratio, (dw, dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "579f9df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t2 - t1 = 0.19247078895568848 seconds\n",
      "[[[1.0079020e+01 4.5858784e+00 5.3581104e+00 ... 1.4807056e-01\n",
      "   2.6016332e-02 3.1623468e-02]\n",
      "  [1.7636999e+01 6.3849373e+00 9.9057484e+00 ... 1.4969431e-01\n",
      "   2.4869159e-02 3.1803496e-02]\n",
      "  [2.4440678e+01 7.6431808e+00 1.3563699e+01 ... 1.4739256e-01\n",
      "   1.8786799e-02 2.1544196e-02]\n",
      "  ...\n",
      "  [5.6790222e+02 6.3302032e+02 6.5908472e+02 ... 2.7607130e-02\n",
      "   2.9355656e-02 3.9201073e-02]\n",
      "  [6.0403583e+02 6.4407703e+02 6.2858954e+02 ... 1.7066147e-02\n",
      "   3.6628943e-02 3.0496929e-02]\n",
      "  [6.2628082e+02 6.4488672e+02 5.8874469e+02 ... 1.3586187e-02\n",
      "   4.2572465e-02 5.0827343e-02]]]\n",
      "(1, 25200, 19)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from openvino.inference_engine import IECore\n",
    "\n",
    "ie = IECore()\n",
    "\n",
    "net = ie.read_network(\n",
    "    model=\"/home/ec2-user/SageMaker/yolov5/runs/train/exp/weights/best_openvino_model/best.xml\",\n",
    "    weights=\"/home/ec2-user/SageMaker/yolov5/runs/train/exp/weights/best_openvino_model/best.bin\",\n",
    ")\n",
    "exec_net = ie.load_network(net, \"CPU\")\n",
    "\n",
    "output_layer_ir = next(iter(exec_net.outputs))\n",
    "input_layer_ir = next(iter(exec_net.input_info))\n",
    "\n",
    "\n",
    "# load an image\n",
    "# Text detection models expects image in BGR format\n",
    "image = cv2.imread(\"./datasets/factory/images/IMG_00000.jpg\")\n",
    "\n",
    "# N,C,H,W = batch size, number of channels, height, width\n",
    "N, C, H, W = net.input_info[input_layer_ir].tensor_desc.dims\n",
    "\n",
    "# Resize image to meet network expected input sizes\n",
    "# resized_image = cv2.resize(image, (W, H))\n",
    "im, ratio, (dw, dh) = letterbox(image, new_shape=(640, 640), stride=32, auto=False)\n",
    "\n",
    "# Reshape to network input shape\n",
    "input_image = im.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
    "input_image = np.expand_dims(input_image, 0)\n",
    "input_image = input_image / 255.0\n",
    "\n",
    "# do inference\n",
    "t1 = time.time()\n",
    "result = exec_net.infer(inputs={input_layer_ir: input_image})\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"t2 - t1 = {} seconds\".format(t2 - t1))\n",
    "# print(result.keys())\n",
    "print(result[\"output\"])\n",
    "print(result[\"output\"].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49c5bcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.16, 0.16) (80.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "print(ratio, (dw, dh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "139b82e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = np.copy(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "\n",
    "def nms(boxes, scores, iou_thresh):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        boxes (Tensor[N, 4])): boxes to perform NMS on. They\n",
    "            are expected to be in ``(x1, y1, x2, y2)`` format with ``0 <= x1 < x2`` and\n",
    "            ``0 <= y1 < y2``.\n",
    "        scores (Tensor[N]): scores for each one of the boxes\n",
    "        iou_thresh (float): discards all overlapping boxes with IoU > iou_threshold\n",
    "\n",
    "    Returns:\n",
    "        Tensor: int64 tensor with the indices of the elements that have been kept\n",
    "        by NMS, sorted in decreasing order of scores\n",
    "    \"\"\"\n",
    "    # （x1、y1）（x2、y2）为box的左上和右下角标\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "\n",
    "    # 每一个候选框的面积\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "\n",
    "    # order是按照score降序排序的，得到的是排序的本来的索引，不是排完序的原数组, ::-1表示逆序\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    temp = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        temp.append(i)\n",
    "        # 计算当前概率最大矩形框与其他矩形框的相交框的坐标\n",
    "        # 由于numpy的broadcast机制，得到的是向量\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.minimum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.maximum(y2[i], y2[order[1:]])\n",
    "\n",
    "        # 计算相交框的面积,注意矩形框不相交时w或h算出来会是负数，需要用0代替\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "\n",
    "        # 计算重叠度IoU\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        # 找到重叠度不高于阈值的矩形框索引\n",
    "        inds = np.where(ovr <= iou_thresh)[0]\n",
    "        # 将order序列更新，由于前面得到的矩形框索引要比矩形框在原order序列中的索引小1，所以要把这个1加回来\n",
    "        order = order[inds + 1]\n",
    "    \n",
    "    return np.array(temp)\n",
    "\n",
    "\n",
    "\n",
    "def non_max_suppression(prediction, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, multi_label=False,\n",
    "                        labels=(), max_det=300):\n",
    "    \"\"\"Runs Non-Maximum Suppression (NMS) on inference results\n",
    "\n",
    "    Returns:\n",
    "         list of detections, on (n,6) tensor per image [xyxy, conf, cls]\n",
    "    \"\"\"\n",
    "\n",
    "    nc = prediction.shape[2] - 5          # number of classes\n",
    "    xc = prediction[..., 4] > conf_thres  # candidates\n",
    "\n",
    "    print(\"number of classes = {}\".format(nc))\n",
    "    print(\"number of candidates = {}\".format(xc))\n",
    "\n",
    "    # Checks\n",
    "    assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
    "    assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
    "\n",
    "    # Settings\n",
    "    min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
    "    max_nms = 30000           # maximum number of boxes into torchvision.ops.nms()\n",
    "    time_limit = 10.0         # seconds to quit after\n",
    "    redundant = True          # require redundant detections\n",
    "    multi_label &= nc > 1     # multiple labels per box (adds 0.5ms/img)\n",
    "\n",
    "    t = time.time()\n",
    "    output = [np.zeros((0, 6))] * prediction.shape[0]\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "        x = x[xc[xi]]  # confidence\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Compute conf\n",
    "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "\n",
    "        # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
    "        box = xywh2xyxy(x[:, :4])\n",
    "\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)        \n",
    "        conf = x[:, 5:].max(axis=1, keepdims=True)\n",
    "        j = x[:, 5:].argmax(axis=1)\n",
    "        j = np.expand_dims(j, axis=-1)\n",
    "        \n",
    "        x = np.concatenate((box, conf, j), axis=-1)\n",
    "        x = x[np.where(conf[:, 0] > conf_thres)]\n",
    "\n",
    "        # Check shape\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "        elif n > max_nms:  # excess boxes\n",
    "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
    "        \n",
    "        i = nms(boxes, scores, iou_thres)  # NMS\n",
    "        \n",
    "        if i.shape[0] > max_det:  # limit detections\n",
    "            i = i[:max_det]\n",
    "\n",
    "        output[xi] = x[i]\n",
    "        if (time.time() - t) > time_limit:\n",
    "            print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1017a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 25200, 19)\n",
      "number of classes = 14\n",
      "number of candidates = [[False False False ... False False False]]\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "pred = result[\"output\"]\n",
    "conf_thres = 0.40\n",
    "iou_thres = 0.45\n",
    "classes = None\n",
    "agnostic_nms = False\n",
    "max_det = 1000\n",
    "print(pred.shape)\n",
    "\n",
    "import torch\n",
    "\n",
    "pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
    "print(pred[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8095a947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = [\n",
    "    \"2D_CODE\", \"Caution\", \"3C\", \"EAC\", \"UL2\", \"WEEE\", \"KC\", \n",
    "    \"ATEX\", \"FM2\", \"Failsafe\", \"RCM\", \"FM\", \"CE\", \"UL1\"]\n",
    "\n",
    "colors = {\n",
    "    \"2D_CODE\": (255, 51, 51), \n",
    "    \"Caution\": (255, 0, 255), \n",
    "    \"3C\": (255, 128, 0), \n",
    "    \"EAC\": (0, 153, 0), \n",
    "    \"UL2\": (200, 153, 89), \n",
    "    \"WEEE\": (10, 29, 199), \n",
    "    \"KC\": (49, 48, 153), \n",
    "    \"ATEX\": (40, 210, 144), \n",
    "    \"FM2\": (182, 255, 40), \n",
    "    \"Failsafe\": (100, 92, 49), \n",
    "    \"RCM\": (255, 80, 153), \n",
    "    \"FM\": (255, 20, 20), \n",
    "    \"CE\": (190, 153, 153), \n",
    "    \"UL1\": (100, 153, 153), \n",
    "}\n",
    "\n",
    "full_path = \"./datasets/factory/images/IMG_00000.jpg\"\n",
    "vis_image = cv2.imread(full_path, cv2.IMREAD_COLOR)\n",
    "height, width, channels = vis_image.shape\n",
    "\n",
    "scale_w, scale_h = ratio\n",
    "\n",
    "detections = pred[0]\n",
    "\n",
    "for det in detections:\n",
    "    x_min = (det[0] - dw) / scale_w\n",
    "    y_min = (det[1] - dh) / scale_h\n",
    "    x_max = (det[2] - dw) / scale_w\n",
    "    y_max = (det[3] - dh) / scale_h\n",
    "    \n",
    "    color = colors[class_names[int(det[-1])]]\n",
    "    color = (color[2], color[1], color[0])\n",
    "    label_info = '{} {:.3f}'.format(class_names[int(det[5])], det[4])\n",
    "    vis_image = cv2.rectangle(vis_image, (int(x_min), int(y_min)), (int(x_max), int(y_max)), color, 5)\n",
    "    vis_image = cv2.putText(vis_image, label_info, (int(x_min), int(y_min)-10), cv2.FONT_HERSHEY_SIMPLEX, 1.0, color, 2, cv2.LINE_AA)\n",
    "    \n",
    "cv2.imwrite(\"./result.png\", vis_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c6f93e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
